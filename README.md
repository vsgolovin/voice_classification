# Бинарный классификатор голосов

 *Tестовое задание для проекта Interactive Speaker Recognition*

**Задание**: Классификатор мужских и женских голосов на базе [LibriTTS](https://arxiv.org/abs/1904.02882)


## Краткое описание репозитория

Все исполняемые файлы находятся в корне, перед запуском `decision_tree.py`, `svm.py` и `train_fc.py` необходимо выполнить предобработку звуковых файлов -- сохранить спектры их Фурье-разложений -- для чего следует запустить `ft_export.py`. Предварительно датасет LibriTTS (а именно файл `SPEAKERS.txt` и папку с какой-либо частью датасета, например `dev-clean`) нужно сохранить в директорию `data/LibriTTS`.

Полный список использованных библиотек находится в `requirements.txt`. Наиболее важные из них:
* `numpy-1.23.1`
* `pytorch-1.12.1` + `torchaudio-0.12.1` + `torchvision-0.13.1`
* `scikit-learn-1.1.2`
* `scipy-1.9.1`
* `matplotlib-3.5.2`
* `pandas-1.4.4`


## Чтение и предобработка данных

Работа с датасетом `LibriTTS` осуществляется с помощью одноименного класса, реализованного в фреймворке *PyTorch*.
Рассматриваемая задача существенно проще чем та, для которой был создан `LibriTTS`, поэтому в качестве датасета можно использовать только его часть.
Я выбрал `dev-clean`, включающий в себя 9 часов речи 20 мужчин и 20 женщин -- этого оказалось вполне достаточно. Эта часть датасета была разделена на 3 выборки -- тренировочную, тестовую и валидационную -- в соотношении 60:20:20. Деление производилось по дикторам -- каждый попадал только в 1 выборку.

Предыдущего опыта работы со звуком у меня нет, курс по цифровой обработке сигналов
начался в этом семестре. Из той информации, что у меня есть, самым разумным
способом классифицировать звуковые файлы мне показался анализ спектров их 
Фурье-разложения. В *PyTorch* (точнее в `torchaudio`) для этого есть удобная
функция `spectrogram`, выполняющая оконное преобразование Фурье (STFT). На первом этапе мне хотелось поработать даже не с таким (двухмерным) представлением сигнала,
а с обычным дискретным преобразованием Фурье, но с постоянным числом частот.
Для этого я решил просто усреднять полученный с помощью STFT сигнал во времени.
На графике ниже показано сравнение такого усреднённого STFT с ДПФ, полученным
с помощью `scipy.fft.fft`.

![сравнение усреднённого STFT с DFT](plots/scipy_torch_comparison.png)

Видно, что основные признаки амплитудного спектра сохранаются. Не сомневаюсь, что
существуют более разумные методы решения такой проблемы, но для рассматриваемой
задачи такой подход мне показался приемлемым.

Чтобы не тратить много времени на STFT я решил сохранить эти спектры в отдельную
директорию `data/arrays`, благо много места они не занимают. Выполнить это можно
с помощью скрипта `ft_export.py`.


## Решение задачи классификации

![средние частоты Фурье-разложения голоса мужчин и женщин](plots/avg_frequencies.png)

![график обучения простой полносвязной нейросетевой модели](plots/fc_model.png)

![график fine-tuning'а предобученной VGG11](plots/cnn_model.png)